# -*- coding: utf-8 -*-
"""SEBI_Portfolio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SJUnR02p_LiW628TmvyDE86eQYYuesMR
"""

import csv
from bs4 import BeautifulSoup
import requests
import time
from google.colab import files

# Base URL to scrape data from
BASE_URL = "https://www.sebi.gov.in/sebiweb/other/OtherAction.do?doPmr=yes"

def get_portfolio_managers(soup):
    """Extract all portfolio manager options from the dropdown"""
    select = soup.find('select', {'name': 'pmrId'})
    if not select:
        return []

    managers = []
    for option in select.find_all('option'):
        if option.get('value') and option['value'] != '-- Select the Portfolio Manager Name --':
            managers.append({
                'id': option['value'],
                'name': option.text.strip()
            })
    return managers

def get_table_data(soup):
    """
    Locate the correct table by verifying the <thead> has:
    'Particulars', 'Domestic Clients', and 'Foreign Clients'.
    Then parse its two <tbody> rows:
      1) No. of unique Clients
      2) Assets under Management (AUM)
    We return a single list-of-lists, each sub-list presumably 14 columns.
    """
    # Find all tables with the known classes
    tables = soup.find_all(
        "table",
        class_="table table-striped table-bordered table-hover background statistics-table"
    )

    if not tables:
        return []

    for table in tables:
        thead = table.find('thead')
        if not thead:
            continue

        # Collect all <th> text in the <thead>
        all_th = thead.find_all('th')
        th_texts = [th.get_text(strip=True) for th in all_th]

        # Check if it has the columns we expect
        if ("Particulars" in th_texts
            and "Domestic Clients" in th_texts
            and "Foreign Clients" in th_texts):

            # This should be our table; now parse its two rows in <tbody>
            tbody = table.find('tbody')
            if not tbody:
                return []

            rows = tbody.find_all('tr')
            if len(rows) < 2:
                # If we don't have at least 2 data rows, treat as no data
                return []

            # Extract columns from row #1 (Clients) and row #2 (AUM)
            row1_cols = [td.get_text(strip=True) for td in rows[0].find_all('td')]
            row2_cols = [td.get_text(strip=True) for td in rows[1].find_all('td')]

            # Typically each row has 8 columns:
            # [ "No. of unique Clients..", PF/EPFO, Corporates, Non-Corp, Non-Res, FPI, Others, Total ]
            # [ "Assets under Management..", PF/EPFO, Corporates, Non-Corp, Non-Res, FPI, Others, Total ]

            # We only want the numeric columns (indexes 1..7) from each row => total 14.
            if len(row1_cols) == 8 and len(row2_cols) == 8:
                combined = row1_cols[1:] + row2_cols[1:]  # 7 + 7 = 14
                return [combined]

    # If we never find a matching table, or if data is incomplete
    return []

def scrape_data(session, pm_id, year, month):
    """Scrape data for specific portfolio manager, year, and month."""
    params = {
        'pmrId': pm_id,
        'year': str(year),
        'month': str(month),
        'doPmr': 'yes'
    }

    try:
        # Add headers to mimic a browser request
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
            'Referer': BASE_URL
        }

        response = session.get(BASE_URL, params=params, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        return get_table_data(soup)
    except Exception as e:
        print(f"Error scraping data for PM: {pm_id}, Year: {year}, Month: {month}")
        print(f"Error details: {str(e)}")
        return []

def write_to_csv(data, filename):
    """Write the collected data to a CSV file."""
    headers = [
        "Portfolio_Manager", "Year", "Month",
        "PF/EPFO_Clients", "Corporates_Clients", "Non-Corporates_Clients",
        "Non-Residents_Clients", "FPI_Clients", "Others_Clients", "Total_Clients",
        "PF/EPFO_AUM", "Corporates_AUM", "Non-Corporates_AUM",
        "Non-Residents_AUM", "FPI_AUM", "Others_AUM", "Total_AUM"
    ]

    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(headers)
        writer.writerows(data)

def main():
    # Create a session to maintain cookies
    session = requests.Session()

    # Get the initial page to extract portfolio managers
    try:
        # Add headers to mimic a browser request
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive'
        }

        print("Fetching initial page...")
        response = session.get(BASE_URL, headers=headers)
        response.raise_for_status()
        print(f"Initial page fetched. Status code: {response.status_code}")

        soup = BeautifulSoup(response.content, 'html.parser')

        # Get all portfolio managers
        portfolio_managers = get_portfolio_managers(soup)
        print(f"Found {len(portfolio_managers)} portfolio managers")

        if not portfolio_managers:
            print("No portfolio managers found! Checking page structure...")
            all_selects = soup.find_all('select')
            print(f"Found {len(all_selects)} select elements on the page")
            for i, select in enumerate(all_selects):
                print(f"Select {i+1} name: {select.get('name', 'No name')}")
                print(f"Select {i+1} options: {len(select.find_all('option'))}")
            return
    except Exception as e:
        print(f"Error fetching initial page: {str(e)}")
        return

    all_data = []
    total_pms = len(portfolio_managers)

    # Years and months to scrape (all months of each year)
    years = range(2022, 2025)
    months = [1, 4, 7, 10]  # 1 to 12

    # Save data periodically in case of interruptions
    intermediate_file = "sebi_portfolio_data_intermediate.csv"

    for i, pm in enumerate(portfolio_managers, 1):
        print(f"Processing Portfolio Manager {i}/{total_pms}: {pm['name']}")

        company_data = []
        for year in years:
            for month in months:
                time.sleep(1)  # Delay to avoid rate limiting
                print(f"Scraping data for {pm['name']}, {year}-{month:02d}...")
                data_rows = scrape_data(session, pm['id'], year, month)
                for row in data_rows:
                    # Combine PM info + year/month + the 14 columns
                    formatted_row = [pm['name'], year, month] + row
                    company_data.append(formatted_row)
                    all_data.append(formatted_row)  # Add to all_data immediately

                print(f"Completed: {year}-{month:02d}")

                # Save intermediate data every 10 requests
                if (i * len(years) * len(months) + (year - years[0]) * len(months) + month) % 10 == 0:
                    write_to_csv(all_data, intermediate_file)
                    print(f"Intermediate data saved to {intermediate_file}")

        if company_data:
            print(f"Successfully scraped data for {pm['name']}. Companies completed: {i}/{total_pms}")
        else:
            print(f"No data found for {pm['name']}")

    if all_data:
        output_filename = "sebi_portfolio_data_complete.csv"
        write_to_csv(all_data, output_filename)
        print(f"\nData successfully written to {output_filename}")

        # Attempt download in Colab
        try:
            files.download(output_filename)
            print("Download initiated. Check your browser's download folder.")
        except Exception as e:
            print(f"Error during download: {e}")
            print("You can manually download the file from Colab's file browser.")
    else:
        print("No data was collected. Please check the website structure or your internet connection.")

if __name__ == "__main__":
    main()